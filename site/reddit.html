<!DOCTYPE html>
<html lang="en">
<head>
    <title>Sentiment Analysis - Reddit Data Collection</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="header_img">
        <img src="images/misc/official-header.png"></img>
    </div>
<div class="menu">
    <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="twitter.html">Twitter</a></li>
        <li><a href="reddit.html">Reddit</a></li>
        <li><a href="instagram.html">Instagram</a></li>
        <li><a href="about.html">About Us</a></li>
        <li><a href="analysis.html">Analysis</a></li>
        <li><a href="timeline.html">Timeline</a></li>
    </ul>
</div>
<main>
    <div class="block">
        <h1>Reddit Data Collection</h1> <h4><i> - by Tyriq Zvijer </i></h4>
        <p class="text-align">Reddit is a community forum-based site that was founded on the 23rd of June in 2005. Each forum has the “r/“ tag before their specific topic. Users can join a wide range of topics that interest them. Useful or well-received content can get awards in the form of platinum, gold, or silver “coins.”</p>
        <h2>Data Collection</h2>
        <p class="text-align">I collected posts from Reddit by scraping the platform. I got access to Reddit’s developer site which allowed me to start using their API. After some testing, I found that the Reddit API had a strict data limitation of 100 posts per query. This did not yield enough data during my initial tests. This meant that I would have to run multiple queries and account for the one’s I had already found. After some research, I found an alternative solution in a python wrapper called PRAW (Python Reddit API Wrapper) that helped me get around this limitation. I set up a list of keywords to search for (“covid”, “covid-19”, “coronavirus”, and “covid19”), as well as a list of subreddits to search through (“AskReddit”, ”Coronavirus”, “collapse”, “politics”, “conspiracy”, “news”, “Conservative”, and “democrats”). I used for loops to look for every keyword within each respective subreddit and limited the amount of posts returned for each subreddit to 50. I decided on this number because of how long my program took to fully search through my query. A limitation of 100 posts was still not done even after 2 full days. 50 posts reduced that time to a little over 16 hours. In total, the algorithm collected 4,606 comments across those 8 subreddits. I consulted Professor Rich Thompson and made sure that the amount of data returned would be sufficient for our analysis. After the data was cleaned and processed, it was then graphed for visualization.</p>
        <h2>Data Visualization</h2>
        <p class="center">All graphs were graphed in R.</p>
        <img class="image" src="images/reddit/NegativeScore-reddit-gpoint.png">
        <img class="image" src="images/reddit/PositiveGraph-reddit-gpoint.png">
        <img class="image" src="images/reddit/Compound_Score-reddit-gpoint.png">
        <img class="image" src="images/reddit/reddit-sntmnt-bgrph.png">
        <h2 class="sub-header no-margin">Word Cloud</h2>
        <img src="images/reddit/wordcloud-reddit-better.png">
        <div class="center">
        <p class="credit">This WordCloud is by Kalyssa Harris</p>
        </div>
    </div>
</main>
    <hr>
    <footer>For ISTA 498 - Capstone by Rincon, Thomas O’Connell, Harris, Zvijer, Barraza</footer>
</body>
</html>